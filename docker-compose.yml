#######################################################
############## EPSI (2025): MSPR AMAZING ##############
##############         Version: 1.0        ############
#######################################################

version: "3.8"

services:
    nodemain:
        build:
            context: .
            dockerfile: Dockerfile
        command: /opt/spark/sbin/start-master.sh
        environment:
            - SPARK_NO_DAEMONIZE=true
            - SPARK_LOCAL_DIRS=/tmp/spark-tmp
        networks:
            - sparknet
        ports:
            - "${SPARK_MASTER_PORT}:8080" # Interface web du Spark master
        volumes:
            - ./data:/opt/spark/data
            - ./logs:/opt/spark/logs
            - ./src:/opt/spark/src
            - ./spark-tmp:/tmp/spark-tmp
            - ./conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
            #- ./conf/log4j2.properties:/opt/spark/conf/log4j2.properties

    worker1:
        build:
            context: .
            dockerfile: Dockerfile
        depends_on:
            nodemain:
                condition: service_started
        command:
            [
                "/opt/spark/bin/spark-class",
                "org.apache.spark.deploy.worker.Worker",
                "spark://nodemain:7077",
            ]
        environment:
            - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}
            - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}
            - SPARK_NO_DAEMONIZE=true
            - SPARK_LOCAL_DIRS=/tmp/spark-tmp
        networks:
            - sparknet
        volumes:
            - ./data:/opt/spark/data
            - ./logs:/opt/spark/logs
            - ./src:/opt/spark/src
            - ./spark-tmp:/tmp/spark-tmp
            - ./conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
            #- ./conf/log4j2.properties:/opt/spark/conf/log4j2.properties
        deploy:
            resources:
                limits:
                    memory: 8G

    worker2:
        build:
            context: .
            dockerfile: Dockerfile
        depends_on:
            nodemain:
                condition: service_started
        command:
            [
                "/opt/spark/bin/spark-class",
                "org.apache.spark.deploy.worker.Worker",
                "spark://nodemain:7077",
            ]
        environment:
            - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}
            - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}
            - SPARK_NO_DAEMONIZE=true
            - SPARK_LOCAL_DIRS=/tmp/spark-tmp
        networks:
            - sparknet
        volumes:
            - ./data:/opt/spark/data
            - ./logs:/opt/spark/logs
            - ./src:/opt/spark/src
            - ./spark-tmp:/tmp/spark-tmp
            - ./conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
            #- ./conf/log4j2.properties:/opt/spark/conf/log4j2.properties
        deploy:
            resources:
                limits:
                    memory: ${WORKER_MAX_MEMORY}

    submitter:
        build:
            context: .
            dockerfile: Dockerfile
        container_name: spark-submit-container
        depends_on:
            nodemain:
                condition: service_started
            worker1:
                condition: service_started
            worker2:
                condition: service_started
        entrypoint: ["/opt/spark/bin/spark-submit"]
        command: ["--master", "spark://nodemain:7077", "src/main.py"]
        ports:
            - "${SPARK_SUBMITTER_PORT}:4040"
        volumes:
            - ./data:/opt/spark/data
            - ./logs:/opt/spark/logs
            - ./src:/opt/spark/src
            - ./spark-tmp:/tmp/spark-tmp
            - ./conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
        environment:
            - SPARK_LOCAL_DIRS=/tmp/spark-tmp
        networks:
            - sparknet

    minio:
        image: minio/minio
        hostname: minio
        container_name: minio
        ports:
            - "9001:9001"
            - "${MINIO_PORT}:9000"
        command: ["server", "/data", "--console-address", ":9001"]
        volumes:
            - ./minio:/data
        environment:
            - MINIO_ROOT_USER=${MINIO_ACCESS_KEY}
            - MINIO_ROOT_PASSWORD=${MINIO_SECRET_KEY}
        healthcheck:
            test: ["CMD", "curl", "-f", "http://minio:9000/minio/health/live"]
            interval: 30s
            timeout: 5s
            retries: 5
        networks:
            - sparknet

    elasticsearch:
        image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
        container_name: elasticsearch
        environment:
            - discovery.type=single-node
            - xpack.security.enabled=false
            - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
            - bootstrap.memory_lock=true
        ulimits:
            memlock:
                soft: -1
                hard: -1
        volumes:
            - ./monitoring/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
            - elasticsearch_data:/usr/share/elasticsearch/data
        ports:
            - "${ES_PORT}:9200"
        networks:
            - sparknet

    logstash:
        image: docker.elastic.co/logstash/logstash:8.12.0
        container_name: logstash
        depends_on:
            - elasticsearch
        environment:
            - LS_JAVA_OPTS=-Xms1g -Xmx1g
            - ES_HOST=${ES_HOST} # Utile pour le spark.conf
            - ES_PORT=${ES_PORT} # Utile pour le spark.conf
        volumes:
            - ./monitoring/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
            - ./monitoring/logstash/config/pipelines.yml:/usr/share/logstash/config/pipelines.yml
            - ./monitoring/logstash/pipeline/spark.conf:/usr/share/logstash/pipeline/spark.conf
            - ./logs:/opt/spark/logs:ro
        networks:
            - sparknet

    grafana:
        image: grafana/grafana:10.4.0
        container_name: grafana
        depends_on:
            - elasticsearch
        environment:
            - GF_SECURITY_ADMIN_USER=${GF_USER}
            - GF_SECURITY_ADMIN_PASSWORD=${GF_USER_PASSWORD}
        ports:
            - "${GF_PORT}:3000"
        volumes:
            - grafana_data:/var/lib/grafana
            - ./grafana/dashboards:/etc/grafana/dashboards
            - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
            - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
        networks:
            - sparknet

volumes:
    spark-tmp:
        driver: local
        driver_opts:
            type: none
            o: bind
            device: ./spark-tmp
    elasticsearch_data:
        driver: local
        driver_opts:
            type: none
            o: bind
            device: ./monitoring/elasticsearch/data
    grafana_data:
        driver: local
        driver_opts:
            type: none
            o: bind
            device: ./monitoring/grafana/data

networks:
    sparknet:
        driver: bridge
        name: sparknet
