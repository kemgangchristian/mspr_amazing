#######################################################
############## EPSI (2025): MSPR AMAZING ##############
##############         Version: 1.0        ############
#######################################################

input {
  file {
    path => "/opt/spark/logs/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => multiline {
      pattern => "^%{TIMESTAMP_ISO8601} "
      negate => true
      what => "previous"
    }
  }
}

filter {
  grok {
    match => { 
      "message" => "^%{TIMESTAMP_ISO8601:log_timestamp} - %{DATA:component} - %{LOGLEVEL:log_level} - %{GREEDYDATA:log_message}" 
    }
  }

  # Extraction des métriques spécifiques
  grok {
    match => {
      "log_message" => [
        "Nombre initial d'enregistrements : %{NUMBER:initial_records:int}",
        "Lignes invalides supprimées : %{NUMBER:invalid_rows_removed:int}",
        "Outliers supprimés : %{NUMBER:outliers_removed:int}",
        "Doublons supprimés : %{NUMBER:duplicates_removed:int}",
        "Nombre final d'enregistrements : %{NUMBER:final_records:int}",
        "Taux de rétention : %{NUMBER:retention_rate:float}%"
      ]
    }
    break_on_match => false
  }

  date {
    #match => ["log_timestamp", "ISO8601"]
    match => ["log_timestamp", "YYYY-MM-dd HH:mm:ss"]
    timezone => "UTC"
    target => "@timestamp"
  }

  mutate {
    remove_field => ["message", "@version", "host"]
    gsub => [
      "log_message", "[\n\t]", " ",
      "log_message", "\s+", " "
    ]
  }
}

output {
  elasticsearch {
    hosts => ["${ES_HOST}:${ES_PORT}"]
    index => "spark-logs-%{+YYYY.MM.dd}"
    document_id => "%{path}-%{@timestamp}"
  }

  # stdout { codec => rubydebug }  # Pour debug dans la console
}
