#######################################################
############## EPSI (2025): MSPR AMAZING ##############
##############         Version: 1.0        ############
#######################################################

input {
  file {
    path => "/opt/spark/logs/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => multiline {
      pattern => "^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}"
      negate => true
      what => "previous"
    }
  }
}

filter {
  # GROK principal : extrait le timestamp, source, niveau de log et message
  grok {
    match => {
      "message" => "^%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND},%{INT:millis} - %{DATA:source} - %{LOGLEVEL:log_level} - %{GREEDYDATA:log_message}"
    }
    tag_on_failure => []
  }

  # Extraction des métriques individuelles
  if "fichiers Parquet valides détectés" in [log_message] {
    grok {
      match => { "log_message" => "%{NUMBER:valid_parquet_files:int} fichiers Parquet valides détectés" }
      tag_on_failure => []
    }
  }

  if "Nombre initial d'enregistrements :" in [log_message] {
    grok {
      match => { "log_message" => "Nombre initial d'enregistrements : %{NUMBER:initial_records:int}" }
      tag_on_failure => []
    }
  }

  if "Doublons supprimés :" in [log_message] {
    grok {
      match => { "log_message" => "Doublons supprimés : %{NUMBER:duplicates_removed:int}" }
      tag_on_failure => []
    }
  }

  if "Nombre final d'enregistrements :" in [log_message] {
    grok {
      match => { "log_message" => "Nombre final d'enregistrements : %{NUMBER:final_records:int}" }
      tag_on_failure => []
    }
  }

  # Ajout d'un identifiant de regroupement pour l'agrégation
  mutate {
    add_field => {
      "task_id" => "%{path}"
    }
  }

  # Agrégation des valeurs par fichier log
  aggregate {
    task_id => "%{some_unique_field}"   # Utilise un champ unique pour agréger (exemple)
    code => "
      map['valid_parquet_files'] ||= 0
      map['initial_records'] ||= 0
      map['duplicates_removed'] ||= 0
      map['final_records'] ||= 0

      map['valid_parquet_files'] += event.get('valid_parquet_files') || 0
      map['initial_records'] += event.get('initial_records') || 0
      map['duplicates_removed'] += event.get('duplicates_removed') || 0
      map['final_records'] += event.get('final_records') || 0

      # Stocke aussi dans l'event pour qu'on puisse y accéder plus tard dans timeout_code
      event.set('valid_parquet_files', map['valid_parquet_files'])
      event.set('initial_records', map['initial_records'])
      event.set('duplicates_removed', map['duplicates_removed'])
      event.set('final_records', map['final_records'])
    "
    timeout => 120   # Par exemple 120 secondes
    timeout_code => "
      event.set('total_valid_parquet_files', event.get('valid_parquet_files') || 0)
      event.set('total_initial_records', event.get('initial_records') || 0)
      event.set('total_duplicates_removed', event.get('duplicates_removed') || 0)
      event.set('total_final_records', event.get('final_records') || 0)
    "
    timeout_task_id_field => "some_unique_field"  # Doit être le même que task_id
    push_map_as_event_on_timeout => true
    inactivity_timeout => 60
  }

  # Nettoyage du timestamp
  date {
    match => ["log_timestamp", "YYYY-MM-dd HH:mm:ss,SSS"]
    timezone => "UTC"
    target => "@timestamp"
  }

  # Nettoyage et normalisation du champ log_message
  mutate {
    gsub => [
      "log_message", "[\n\t]", " ",
      "log_message", "\s+", " "
    ]
    remove_field => ["message", "@version", "host", "millis"]
  }

  # Garder uniquement les événements agrégés finaux
  if ![total_valid_parquet_files] {
    drop {}
  }
}

output {
  elasticsearch {
    hosts => ["${ES_HOST}:${ES_PORT}"]
    index => "spark-logs-%{+YYYY.MM.dd}"
    document_id => "%{path}-%{@timestamp}"
  }

  # stdout { codec => rubydebug }  # Décommente pour tester dans la console
}